{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO9u2jy1Di0TrHgWJmrBdzB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JpX-uaF0X37e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674415837723,"user_tz":180,"elapsed":27171,"user":{"displayName":"Paulo Cesar De Morais Sousa","userId":"07081134471429623581"}},"outputId":"712ddd9c-c1a4-41e9-f9ef-f3e3a4d2975b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","source":["!pip install scikit-learn\n","!pip install scikit-learn-extra\n","!pip install nltk\n","!pip install path"],"metadata":{"id":"61f-tp57X7RN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674415856016,"user_tz":180,"elapsed":18304,"user":{"displayName":"Paulo Cesar De Morais Sousa","userId":"07081134471429623581"}},"outputId":"41b197d8-fe5a-4f63-8681-7a6575160ca1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n","Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scikit-learn-extra\n","  Downloading scikit_learn_extra-0.2.0-cp38-cp38-manylinux2010_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn-extra) (1.7.3)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn-extra) (1.21.6)\n","Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn-extra) (1.0.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.1.0)\n","Installing collected packages: scikit-learn-extra\n","Successfully installed scikit-learn-extra-0.2.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting path\n","  Downloading path-16.6.0-py3-none-any.whl (26 kB)\n","Installing collected packages: path\n","Successfully installed path-16.6.0\n"]}]},{"cell_type":"code","source":["import nltk\n","\n","# Punkt é necessário para realizar a tokenização de palavras e sentenças\n","nltk.download('punkt') "],"metadata":{"id":"Qf8et3v20LbB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674415858311,"user_tz":180,"elapsed":2303,"user":{"displayName":"Paulo Cesar De Morais Sousa","userId":"07081134471429623581"}},"outputId":"6b235862-39d7-4643-ae39-af5924b82131"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["from sklearn_extra.cluster import KMedoids\n","import numpy as np\n","import re\n","from path import Path\n","\n","documento  = []\n","notas      = []\n","diretorio  = Path('/content/drive/MyDrive/Proj_PFC/Corpus_TXT_PFC2/Hotel4/')\n","nomesumario = diretorio.split('/')\n","\n","# Usa a pasta de onde os documentos foram retirados como nome do sumário\n","nomesumario = nomesumario[len(nomesumario)-2]\n","#print(nomesumario)\n","\n","# Carrega documentos para serem sumarizados. Entretanto, os testes iniciais\n","# são feitos com sentenças pré-selecionadas, pois as técnicas Opizer-E e Tadano\n","# trabalham dessa forma em suas implementações por Condori\n","for arquivos in diretorio.files('*.txt'):\n","    with arquivos.open(mode='rt', encoding='utf-8') as leitor:\n","        documento = leitor.readlines()\n","\n","# Vetor de nota previamente utilizado para verificar a polaridade das sentenças.\n","# Não será utilizado até então pois o foco do LexRank é em extrair sentenças sem\n","# verificar a polaridade.\n","#for arquivos in diretorio.files('*.score'):\n","    #with arquivos.open(mode='rt', encoding='utf-8') as leitor:\n","        #notas = leitor.readlines()\n","\n","\n","#print(documento)\n","\n","# Salva as opinioes em um formato lista de listas, para separar sentenças por\n","# opiniões e facilitar a classificação de polaridade futuramente, além de tornar\n","# a representação visual delas mais concisa durante o processamento.\n","opinioes_temp = []\n","opinioes = []\n","sentenca = 0\n","\n","# Classifica as sentenças, sempre estando no índice 0 já que as sentenças \n","# classificadas são removidas do documento inicial, portanto seu tamanho \n","# diminui até 0\n","while sentenca < len(documento): \n","    #print(sentenca, \" | documento_len = \", len(documento) )\n","    #print(documento[sentenca])\n","    if(documento[sentenca] == '---\\n'): # Se essa linha for um delimitador\n","      documento.remove(documento[sentenca]) # Remova o delimitador\n","      opinioes.append(opinioes_temp) # Adicione a opinião à lista principal\n","      opinioes_temp = [] # Esvazia o vetor de sentenças da opinião processada\n","    else: # Se não for um delimitador\n","      opinioes_temp.append(documento[sentenca]) # Adicione a sentença à opinião\n","      documento.remove(documento[sentenca]) # Remova-a do documento recebido\n","\n","#print(opinioes)\n","\n","#\n","# Pré-processamento textual do conteúdo passado como entrada\n","#\n","\n","# Coloca todas as palavras em minúsculo para evitar repetição de tokens\n","for i in range(0, len(opinioes)):\n","  for j in range(0, len(opinioes[i])):\n","    opinioes[i][j] = opinioes[i][j].lower()\n","\n","# Remove pontuações do texto, para evitar que estes símbolos se tornem tokens\n","for i in range(0, len(opinioes)):\n","  for j in range(0, len(opinioes[i])):\n","    opinioes[i][j] = re.sub(r'[^\\w\\s]', '', opinioes[i][j])\n","\n","#print(\"Sentenças separadas por opinião:\", opinioes, '\\n')\n","\n","\n","\n","#\n","# Passo 1: Tokenização de palavras\n","#\n","tokens_palavras = []\n","tokens_temp = []\n","\n","# Cria uma lista de listas, onde cada lista interna representa os tokens de uma\n","# opinião como um todo, não fazendo separação por sentenças até o momento.\n","for i in range(0, len(opinioes)):\n","  for j in range(0, len(opinioes[i])):\n","    tokens_temp.extend(nltk.word_tokenize(opinioes[i][j]))\n","  tokens_palavras.append(tokens_temp)\n","  tokens_temp = []\n","\n","#print(\"Tokens das sentenças:\", tokens_palavras, '\\n')\n","\n","\n","\n","#\n","# Passo 2: Remoção de stopwords\n","#\n","\n","# Carrega as stopwords do arquivo salvo na pasta de recursos\n","arq_stopwords = open('/content/drive/MyDrive/Proj_PFC/KMedoids/recursos/stopwords_pt.txt', 'rt')\n","stopwords = arq_stopwords.readlines()\n","\n","# Remove caracteres de nova linha no final de cada stopword do arquivo\n","for i in range(0, len(stopwords)):\n","  stopwords[i] = stopwords[i].rstrip('\\n')\n","\n","#print(\"Stopwords:\", stopwords, '\\n')\n","\n","\n","tokens_sem_stopwords = []\n","stopword_temp = []\n","\n","for i in range (0, len(tokens_palavras)):\n","  stopword_temp = []\n","  for palavra in tokens_palavras[i]:\n","    if not(palavra in stopwords):\n","        stopword_temp.append(palavra)\n","  tokens_sem_stopwords.append(stopword_temp)\n","  \n","#print(\"Tokens sem stopwords:\", tokens_sem_stopwords, '\\n')\n","\n","\n","\n","#\n","# Passo 3: Cálculo de frequências por TF-IDF.\n","#\n","### Implementação do TF-IDF baseada em: https://www.computersciencemaster.com.br/como-implementar-o-tf-idf-em-python/\n","\n","term_freqs = {}\n","inv_doc_freqs = {}\n","\n","# Cálculo do Term Frequency\n","for i in range(0, len(tokens_sem_stopwords)):\n","  for palavra in tokens_sem_stopwords[i]:\n","    if palavra not in term_freqs.keys():\n","      term_freqs[palavra] = 1\n","    else:\n","      term_freqs[palavra] += 1\n","\n","#print(\"TF das palavras:\", term_freqs, '\\n')\n","\n","\n","# Cálculo do Inverse Document Frequency\n","\n","num_docs = 0\n","\n","# Cada opinião é tratada como um documento, então incremente num_docs caso o \n","# termo esteja presente num dos elementos do vetor de tokens a nível de opinião\n","for term in term_freqs:\n","  num_docs = 0\n","\n","  for i in range(0, len(tokens_sem_stopwords)):\n","    if term in tokens_sem_stopwords[i]:\n","      num_docs += 1\n","\n","  inv_doc_freqs[term] = np.log((len(tokens_sem_stopwords)/num_docs)+1)\n","\n","#print(\"IDF das palavras:\", inv_doc_freqs)\n","\n","\n","# Calculo da matriz TF para as palavras encontradas.\n","\n","\n","matriz_tf = {}\n","tf_doc = []\n","doc_frequency = 0\n","num_tokens = 0\n","\n","for i in range(0, len(tokens_sem_stopwords)):\n","  num_tokens += len(tokens_sem_stopwords[i])\n","\n","for term in term_freqs:\n","  tf_doc = []\n","  for i in range(0, len(tokens_sem_stopwords)):\n","    doc_frequency = 0\n","    for word in tokens_sem_stopwords[i]:\n","      if word == term:\n","        doc_frequency += 1\n","    tf_term = doc_frequency/num_tokens\n","    tf_doc.append(tf_term)\n","\n","  matriz_tf[term] = tf_doc\n","\n","#print(\"Matriz tf de tamanho[\",len(matriz_tf),\"]\\n\")\n","\n","\n","# Cálculo final do TF-IDF\n","\n","score = 0\n","tf_idf = []\n","matriz_tf_idf = []\n","\n","for term in matriz_tf.keys():\n","  score = 0\n","  tf_idf = []\n","  for value in matriz_tf[term]:\n","    score = value * inv_doc_freqs[term]\n","    tf_idf.append(score)\n","  matriz_tf_idf.append(tf_idf)\n","#print(\"Matriz tf_idf de tamanho[\",len(matriz_tf_idf),\"][\",len(matriz_tf_idf[0]),\"]\\n\")\n","\n","\n","\n","#\n","#Passo 4: A ser realizado, caso seja necessário\n","#\n"],"metadata":{"id":"9ZoXNwS6Zjd_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#\n","#\n","# Implementação do KMedoids\n","#\n","#\n","\n","# kmedoids_temp é um arquivo usado para gravar dados de teste da execução do \n","# algoritmo\n","#kmedoids_temp = open('/content/drive/MyDrive/Proj_PFC/KMedoids/temp/kmedoids_temp.txt', 'w')\n","num_clusters = 49 # Número máximo de agrupamentos e sentenças a ser processado\n","mat_sentencas = np.asarray(matriz_tf_idf)\n","no_match = 0\n","palavras_centrais = []\n","palavras_centrais_ind = []\n","sent_score = [] # [opinião][sentença]\n","\n","for i in range(0, len(opinioes)):\n","  sent_score_temp = []\n","  for j in range(0, len(opinioes[i])):\n","    sent_score_temp.append(0)\n","  sent_score.append(sent_score_temp)\n","\n","sumarizador = KMedoids(n_clusters=num_clusters, metric='euclidean', method='pam', init='heuristic', max_iter=5000, random_state=None).fit(mat_sentencas)\n","#print(sumarizador.cluster_centers_) # Medoids do agrupamento\n","\n","#kmedoids_temp.write(\"Medoids:\")\n","#kmedoids_temp.write(str(sumarizador.cluster_centers_))\n","#kmedoids_temp.write('\\n')\n","\n","# Encontra as palavras que compõem os medoids descobertos e as salva em índices\n","# específicos com base no índice do medoid\n","for i in range(0, len(sumarizador.cluster_centers_)):\n","  palavras_centrais_ind = []\n","  for j in range(0, len(matriz_tf_idf)):\n","    no_match = 0\n","    for k in range(0, len(matriz_tf_idf[j])):\n","      if matriz_tf_idf[j][k] != sumarizador.cluster_centers_[i][k]:\n","        no_match = 1\n","      \n","    if(no_match == 0):\n","      #print(matriz_tf.get(j))\n","      palavras_centrais_ind.append(list(matriz_tf.keys())[j])\n","  \n","  palavras_centrais.append(palavras_centrais_ind)\n","\n","#print(palavras_centrais)\n","\n","#kmedoids_temp.write(\"Palavras centrais de acordo com os medoids:\")\n","#kmedoids_temp.write(str(palavras_centrais))\n","#kmedoids_temp.write('\\n')\n","\n","# Cruza essas palavras com as sentenças presentes nas opiniões passadas como \n","# entrada e calcula a pontuação delas de acordo com a \n","\n","for i in range(0, len(palavras_centrais)):\n","  for j in range(0, len(opinioes)):\n","    for k in range(0, len(opinioes[j])):\n","      if any(word in opinioes[j][k] for word in palavras_centrais[i]):\n","      #if word in palavras_centrais[i][j] in opinioes[k]:\n","        sent_score[j][k] += 1\n","      #print(opinioes[j][k])\n","\n","#print(\"Pontuações das sentenças de cada opinião:\", sent_score)\n","\n","#kmedoids_temp.write(\"Pontuação das sentenças de acordo com o kmedoids:\")\n","#kmedoids_temp.write(str(sent_score))\n","#kmedoids_temp.write('\\n')\n","\n","# Pega o índice da sentença com maior número de pontos de cada opinião para \n","# adicionar ao sumário final\n","\n","sentencas_centrais = []\n","indice_sentencas_centrais = []\n","\n","for i in range(0, len(sent_score)):\n","  sentencas_centrais.append(0)\n","  indice_sentencas_centrais.append(0)\n","  for j in range(0, len(sent_score[i])):\n","    if sent_score[i][j] > sentencas_centrais[i]:\n","      sentencas_centrais[i] = sent_score[i][j]\n","      indice_sentencas_centrais[i] = j\n","\n","#print(\"Índice das sentenças centrais:\", indice_sentencas_centrais)\n","\n","# Gera o sumário final\n","\n","#sumario_pos   = []\n","#sumario_neg   = []\n","sumario_final = []\n","\n","for i in range(0, num_clusters):\n","  sumario_final.append(opinioes[i][indice_sentencas_centrais[i]])\n","\n","### Classificação de polaridade ainda não será contemplada!!!\n","#sumario_final.append(\"Opiniões positivas:\\n\\n\")\n","#for i in range(0, len(opinioes) ):\n","#  if(int(notas[i]) > 3):\n","#    sumario_final.append(opinioes[i][indice_sentencas_centrais[i]])\n","#    sumario_final.append('\\n')\n","\n","#sumario_final.append(\"\\n\\nOpiniões negativas:\\n\\n\")\n","#for i in range(0, len(opinioes) ):\n","#  if(int(notas[i]) < 3):\n","#    sumario_final.append(opinioes[i][indice_sentencas_centrais[i]])\n","#    sumario_final.append('\\n')\n","\n","#sumario_final.append(\"\\n\\nOpiniões neutras:\\n\\n\")\n","#for i in range(0, len(opinioes) ):\n","#  if(int(notas[i]) == 3):\n","#    sumario_final.append(opinioes[i][indice_sentencas_centrais[i]])\n","#    sumario_final.append('\\n')\n","\n","arq_sum = open('/content/drive/MyDrive/Proj_PFC/KMedoids/Sumarios_PFC2/' + nomesumario + '_KMedoids.sum', 'w')\n","\n","for i in range(0, len(sumario_final)):\n","  arq_sum.write(sumario_final[i])\n","\n","#sumarizador.predict([[0,1], [1,2]])"],"metadata":{"id":"cVI0ROQq-0h_"},"execution_count":null,"outputs":[]}]}